{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Droid\n",
    "\n",
    "This is a notebook to experiment with different methods of getting news sentiment for a given coin.\n",
    "\n",
    "It is assoaciated with a [Blog post on ProfitView](https://profitviews.net/blog/what-i-learned-when-building-an-ai-news-trading-bot).  You can [sign-up there](https://profitview.net/register) to run a bot that trades using news sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin = \"Bitcoin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Sources\n",
    "\n",
    "I wanted to experiment with different news sources to see how they compare.  I chose a few popular ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The [GDELT](https://www.gdeltproject.org/) Project \n",
    "\n",
    "Unfortunately kept getting rate limited by GDELT.  No query went through - even with a small query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDELT API endpoint for news\n",
    "base_url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "\n",
    "# Parameters for GDELT query\n",
    "params = \\\n",
    "    {   'query': coin\n",
    "    ,   'format': 'json'\n",
    "    ,   'maxrecords': 5  # GDELT allows up to 250 records\n",
    "    ,   'timespan': '15m'\n",
    "    ,   'sort': 'DateDesc'  # Sort by date descending\n",
    "    ,   'headers': {'User-agent': 'news bot 0.1'}\n",
    "    }\n",
    "\n",
    "gdelt_data = \"\"\n",
    "# Make the request\n",
    "response = requests.get(base_url, params=params)\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        gdelt_data = response.json()\n",
    "        # Print formatted JSON\n",
    "        print(json.dumps(gdelt_data, indent=2))\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON. Response might not be in JSON format.\")\n",
    "        print(\"Response text:\", response.text)\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "    print(\"Response text:\", response.text)\n",
    "\n",
    "# Print formatted JSON\n",
    "print(json.dumps(gdelt_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [News API](https://newsapi.org/)\n",
    "\n",
    "This works well, but is delayed 24 hours unless on a paid plan, which is $449/month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # Store the News API key in your .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key=os.getenv(\"NEWS_API_KEY\"))\n",
    "newsapi_articles = newsapi.get_everything(q='bitcoin',\n",
    "    from_param=datetime.now() - timedelta(days=2),  # On the free plan, data is delayed 24 hours and only 1 day of data is available\n",
    "    sort_by='popularity')\n",
    "newsapi_headlines = [article['title'] for article in newsapi_articles[\"articles\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### [RSS feeds](https://en.wikipedia.org/wiki/RSS)\n",
    "\n",
    "This is free, and Google News itself provides RSS feeds.\n",
    "\n",
    "I used Google's `tbs` parameter to restrict news relevancy to the last hour: `qdr:h`.  This means that older news is promoted only if it's popular within the last hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_url = f\"https://news.google.com/rss/search?q={coin}&tbs=qdr:h\"  # News focus is on the last hour (though older news is promoted if it's popular)\n",
    "\n",
    "feed = feedparser.parse(feed_url)\n",
    "headlines = [{ \"title\": entry.title, \"published\": entry.published } for entry in feed.entries]\n",
    "    \n",
    "print(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "I used two different libraries to get sentiment scores, and OpenAI for an LLM-based approach.  VADER is a rule-based sentiment analysis tool, and TextBlob is a lexicon-based approach combined with a simple machine learning classifier for sentiment analysis.\n",
    "\n",
    "Everything was done using Google News RSS feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [VADER](https://github.com/cjhutto/vaderSentiment)\n",
    "\n",
    "Rather than using a LLM, we can use VADER to get a sentiment score.  VADER (Valence Aware Dictionary and sEntiment Reasoner) is a rule-based sentiment analysis tool designed specifically for short pieces of text such as social media posts, headlines, and reviews. VADER works by combining a lexicon of sentiment-laden words with rules that account for the impact of things like punctuation, capitalization, modifiers, and negations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "vader_title_scores = [(headline['title'], sid.polarity_scores(headline['title'])['compound']) for headline in headlines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "\n",
    "Unlike VADER, which is rule-based, TextBlob uses a lexicon-based approach combined with a simple machine learning classifier for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "textblob_title_scores = [TextBlob(headline['title']).sentiment.polarity for headline in headlines]\n",
    "\n",
    "# Calculate overall sentiment\n",
    "avg_sentiment = sum(textblob_title_scores) / len(textblob_title_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4o-mini Per Headline\n",
    "\n",
    "In order to compare with the other methods, we need to apply the GPT-4o-mini to each headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "load_dotenv()  # Store the API key in your .env file\n",
    "\n",
    "GPT_API_KEY = os.getenv(\"GPT_API_KEY\") \n",
    "client = openai.OpenAI(api_key=GPT_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_title_scores = [float(client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a cryptocurrency trading expert.\"}, \n",
    "              {\"role\": \"user\", \"content\": \n",
    "               f\"\"\"Assess this news headline as it pertains to cryptocurrency {coin}: \n",
    "               {article['title']}. Provide a float between -1 and 1, where -1 is extremely negative, 0 is neutral, and 1 is extremely positive. Provide only the float, no other text.\"\"\"}]\n",
    ").choices[0].message.content) for article in headlines]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the scores\n",
    "\n",
    "It is instructive to compare the scores.  VADER and TextBlob use different methods to get a sentiment score, however neither can take context into account.  GPT-4o-mini should be the most accurate (because it has context), but it is also the most resource-intensive and therefore slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate DataFrames for each sentiment score\n",
    "vader_df = pd.DataFrame(vader_title_scores, columns=['title', 'vader_score'])\n",
    "textblob_df = pd.DataFrame({\n",
    "    'title': [headline['title'] for headline in headlines],\n",
    "    'textblob_score': textblob_title_scores\n",
    "})\n",
    "gpt_df = pd.DataFrame({\n",
    "    'title': [headline['title'] for headline in headlines],\n",
    "    'gpt_score': gpt_title_scores\n",
    "})\n",
    "# Merge DataFrames on title\n",
    "compare_title_scores = vader_df.merge(textblob_df, on='title').merge(gpt_df, on='title')\n",
    "\n",
    "# Style and display\n",
    "styled_df = compare_title_scores.style.format({\n",
    "    'vader_score': '{:.3f}',\n",
    "    'textblob_score': '{:.3f}',\n",
    "    'gpt_score': '{:.3f}'\n",
    "}).set_properties(**{\n",
    "    'text-align': 'left',\n",
    "    'white-space': 'pre-wrap',\n",
    "    'max-width': '500px'\n",
    "}).set_table_styles([\n",
    "    {'selector': 'td', 'props': [('max-width', '500px'), ('white-space', 'pre-wrap')]},\n",
    "    {'selector': 'th', 'props': [('max-width', '500px'), ('white-space', 'pre-wrap')]}\n",
    "])\n",
    "\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o-mini for the entire feed\n",
    "\n",
    "Because repetitive use of the GPT-4o-mini API is slow, I wanted to see if I could get a good score using a single query.  I used the Google News RSS feed, and the `tbs` parameter to restrict news relevancy to the last hour: `qdr:h`.  This means that older news is promoted only if it's popular within the last hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_feed_query = f\"\"\"Assess these news headlines as they pertain to cryptocurrency {coin}, taking into account their publication date and time for relevance. \n",
    "\t\t\t\t\t\tProvide a single floating point number between -1.0 and 1.0, with -1.0 signifying extreme negativity, 0.0 neutrality and 1.0 extreme positivity.\n",
    "\t\t\t\t\t\tProvide only the number and no other text: \n",
    "                        \n",
    "                        \"\"\"\n",
    "\n",
    "google_feed_headlines = \"\".join([f\"Published: {headline['published']}. Headline: {headline['title']}\\n\" for headline in headlines])\n",
    "\n",
    "google_feed_query += google_feed_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a cryptocurrency trading expert.\"}, {\"role\": \"user\", \"content\": google_feed_query}]\n",
    ")\n",
    "response.choices[0].message.content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
